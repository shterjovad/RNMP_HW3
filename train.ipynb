{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a57ad75c-1491-41f3-9a33-e91c7d534ac6",
   "metadata": {},
   "source": [
    "## Split data for online and offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf9ac741-28be-4a0c-be59-10d9c81bc395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_path = \"./raw_data/diabetes_binary_health_indicators_BRFSS2015.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "label_column = 'Diabetes_binary'\n",
    "\n",
    "# Split the data into features and target\n",
    "X = df  # Replace 'label' with your actual target column name\n",
    "y = df[label_column]  # Replace 'label' with your actual target column name\n",
    "\n",
    "# Stratified split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "X_test.to_csv(\"split_data/offline.csv\", index=False)\n",
    "X_test.to_csv(\"split_data/online.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aa717b9-bb7d-4901-b99c-f1348bd99cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/22 01:26:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, count, when, col\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier, DecisionTreeClassifier, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef4b12d-05cb-45a5-a71c-e1fecb259e62",
   "metadata": {},
   "source": [
    "## Read offline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9ad740a-939a-4b3f-853b-cc3f94abb4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = spark.read.csv(\n",
    "    \"split_data/offline.csv\",\n",
    "    sep=\",\", inferSchema=True, header=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3a83db3-d83c-4503-b5b2-897b9ab903dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+--------+---------+---+------+------+--------------------+------------+------+-------+-----------------+-------------+-----------+-------+--------+--------+--------+---+---+---------+------+\n",
      "|Diabetes_binary|HighBP|HighChol|CholCheck|BMI|Smoker|Stroke|HeartDiseaseorAttack|PhysActivity|Fruits|Veggies|HvyAlcoholConsump|AnyHealthcare|NoDocbcCost|GenHlth|MentHlth|PhysHlth|DiffWalk|Sex|Age|Education|Income|\n",
      "+---------------+------+--------+---------+---+------+------+--------------------+------------+------+-------+-----------------+-------------+-----------+-------+--------+--------+--------+---+---+---------+------+\n",
      "|              0|     0|       0|        0|  0|     0|     0|                   0|           0|     0|      0|                0|            0|          0|      0|       0|       0|       0|  0|  0|        0|     0|\n",
      "+---------------+------+--------+---------+---+------+------+--------------------+------------+------+-------+-----------------+-------------+-----------+-------+--------+--------+--------+---+---+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for nulls in each column\n",
    "null_counts = df_data.select([count(when(col(c).isNull(), c)).alias(c) for c in df_data.columns])\n",
    "\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "165fed18-e85c-41c4-94d0-640f762dafcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique count in Diabetes_binary: 2\n",
      "Unique count in HighBP: 2\n",
      "Unique count in HighChol: 2\n",
      "Unique count in CholCheck: 2\n",
      "Unique count in BMI: 77\n",
      "Unique count in Smoker: 2\n",
      "Unique count in Stroke: 2\n",
      "Unique count in HeartDiseaseorAttack: 2\n",
      "Unique count in PhysActivity: 2\n",
      "Unique count in Fruits: 2\n",
      "Unique count in Veggies: 2\n",
      "Unique count in HvyAlcoholConsump: 2\n",
      "Unique count in AnyHealthcare: 2\n",
      "Unique count in NoDocbcCost: 2\n",
      "Unique count in GenHlth: 5\n",
      "Unique count in MentHlth: 31\n",
      "Unique count in PhysHlth: 31\n",
      "Unique count in DiffWalk: 2\n",
      "Unique count in Sex: 2\n",
      "Unique count in Age: 13\n",
      "Unique count in Education: 6\n",
      "Unique count in Income: 8\n"
     ]
    }
   ],
   "source": [
    "for column in df_data.columns:\n",
    "    unique_count = df_data.select(column).distinct().count()\n",
    "    print(f\"Unique count in {column}: {unique_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bd6d641-bc4b-4492-8c76-b229f9573985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before removing duplicates: 50736\n",
      "Number of rows after removing duplicates: 48713\n"
     ]
    }
   ],
   "source": [
    "# Count rows before removing duplicates\n",
    "print(\"Number of rows before removing duplicates:\", df_data.count())\n",
    "\n",
    "# Remove completely identical rows\n",
    "df_no_duplicates = df_data.distinct()\n",
    "\n",
    "# Count rows after removing duplicates\n",
    "print(\"Number of rows after removing duplicates:\", df_no_duplicates.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4005d915-0eca-46a1-8753-cba2535d23f9",
   "metadata": {},
   "source": [
    "## feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c57ba168-902b-4c71-ba83-cf79a99a50c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df_no_duplicates.drop(*[\"Fruits\" , \"Veggies\" , \"Sex\" , \"CholCheck\" , \"AnyHealthcare\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6aee8096-416a-4737-b07b-292ab0f8b34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = df_features.columns\n",
    "feature_columns.remove(label_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea84fba2-ec54-4dbd-86db-a016ef47a085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HighBP',\n",
       " 'HighChol',\n",
       " 'BMI',\n",
       " 'Smoker',\n",
       " 'Stroke',\n",
       " 'HeartDiseaseorAttack',\n",
       " 'PhysActivity',\n",
       " 'HvyAlcoholConsump',\n",
       " 'NoDocbcCost',\n",
       " 'GenHlth',\n",
       " 'MentHlth',\n",
       " 'PhysHlth',\n",
       " 'DiffWalk',\n",
       " 'Age',\n",
       " 'Education',\n",
       " 'Income']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "480d3706-4c52-41f3-bd7e-596df6debb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "data = assembler.transform(df_features).select(col(\"features\"), col(label_column).alias(\"label\"))\n",
    "(train_data, test_data) = data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d30e2b-1768-41b9-9fee-3e669e03000d",
   "metadata": {},
   "source": [
    "## Models Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8add17d1-1494-4836-8d31-df936553b55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/22 00:21:58 WARN DAGScheduler: Broadcasting large task binary with size 1306.1 KiB\n",
      "24/01/22 00:21:59 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/01/22 00:21:59 WARN DAGScheduler: Broadcasting large task binary with size 1232.6 KiB\n",
      "24/01/22 00:22:02 WARN DAGScheduler: Broadcasting large task binary with size 1310.1 KiB\n",
      "24/01/22 00:22:02 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/01/22 00:22:02 WARN DAGScheduler: Broadcasting large task binary with size 1228.6 KiB\n",
      "24/01/22 00:22:05 WARN DAGScheduler: Broadcasting large task binary with size 1287.3 KiB\n",
      "24/01/22 00:22:05 WARN DAGScheduler: Broadcasting large task binary with size 2025.2 KiB\n",
      "24/01/22 00:22:05 WARN DAGScheduler: Broadcasting large task binary with size 1198.6 KiB\n",
      "24/01/22 00:22:08 WARN DAGScheduler: Broadcasting large task binary with size 1351.0 KiB\n",
      "24/01/22 00:22:08 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n"
     ]
    }
   ],
   "source": [
    "# Define models\n",
    "rf = RandomForestClassifier()\n",
    "lr = LogisticRegression()\n",
    "gbt = GBTClassifier()\n",
    "\n",
    "# Define parameter grids for each model\n",
    "paramGrid_rf = ParamGridBuilder().addGrid(rf.maxDepth, [5, 10]).build()\n",
    "paramGrid_lr = ParamGridBuilder().addGrid(lr.maxIter, [10, 100]).build()\n",
    "paramGrid_gbt = ParamGridBuilder().addGrid(gbt.maxIter, [10, 20]).build()\n",
    "\n",
    "# Define evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"f1\")\n",
    "\n",
    "# Cross-validation for each model\n",
    "crossval_rf = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid_rf, evaluator=evaluator, numFolds=3)\n",
    "crossval_lr = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid_lr, evaluator=evaluator, numFolds=3)\n",
    "crossval_gbt = CrossValidator(estimator=gbt, estimatorParamMaps=paramGrid_gbt, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "# Fit models\n",
    "cvModel_rf = crossval_rf.fit(train_data)\n",
    "cvModel_lr = crossval_lr.fit(train_data)\n",
    "cvModel_gbt = crossval_gbt.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48574182-00fa-41bc-892c-91be6e897bf3",
   "metadata": {},
   "source": [
    "## Evaluation of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5a13d5d5-1007-4e62-ae88-01d8abc92716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/22 00:23:09 WARN DAGScheduler: Broadcasting large task binary with size 1272.7 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest F1 Score:  0.8031765264076465\n",
      "Logistic Regression F1 Score:  0.8112380324329342\n",
      "Gradient-Boosted Trees F1 Score:  0.8129871105315047\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions on test data\n",
    "predictions_rf = cvModel_rf.transform(test_data)\n",
    "predictions_lr = cvModel_lr.transform(test_data)\n",
    "predictions_gbt = cvModel_gbt.transform(test_data)\n",
    "\n",
    "# Evaluate models\n",
    "f1_score_rf = evaluator.evaluate(predictions_rf)\n",
    "f1_score_lr = evaluator.evaluate(predictions_lr)\n",
    "f1_score_gbt = evaluator.evaluate(predictions_gbt)\n",
    "\n",
    "print(\"Random Forest F1 Score: \", f1_score_rf)\n",
    "print(\"Logistic Regression F1 Score: \", f1_score_lr)\n",
    "print(\"Gradient-Boosted Trees F1 Score: \", f1_score_gbt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772458d3-50a9-4d5f-b44d-c935d9851f3a",
   "metadata": {},
   "source": [
    "Clearly the Logistic Regression seems to be better here, so saving that as the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "937b797a-88d0-4528-8843-e86727ffadfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.write().overwrite().save(\"model/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
